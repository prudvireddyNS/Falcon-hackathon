{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "persist_directory = 'docs/chroma/'\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://docs.crewai.com/tools/PGSearchTool/\n",
      "Using Tool: Read website content\n",
      "Scraping https://docs.crewai.com/how-to/Force-Tool-Ouput-as-Result/\n",
      "Using Tool: Read website content\n",
      "Scraping https://twitter.com/joaomdmoura\n",
      "Using Tool: Read website content\n",
      "Scraping https://docs.crewai.com/tools/BrowserbaseLoadTool/\n",
      "Using Tool: Read website content\n",
      "Scraping https://github.com/joaomdmoura/crewAI-examples/tree/main/landing_page_generator\n",
      "Using Tool: Read website content\n"
     ]
    }
   ],
   "source": [
    "from fns import extract_content\n",
    "\n",
    "content = extract_content('https://docs.crewai.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://docs.crewai.com/tools/PGSearchTool/',\n",
       "  'content': 'PG RAG Search - crewAI\\n Skip to content\\n crewAI\\n PG RAG Search\\n Initializing search\\n crewAI\\n crewAI\\n crewAI\\n Home\\n Core Concepts\\n Core Concepts\\n Agents\\n Tasks\\n Tools\\n Processes\\n Crews\\n Collaboration\\n Training\\n Memory\\n Planning\\n Testing\\n Using LangChain Tools\\n Using LlamaIndex Tools\\n How to Guides\\n How to Guides\\n Starting Your crewAI Project\\n Installing CrewAI\\n Getting Started\\n Create Custom Tools\\n Using Sequential Process\\n Using Hierarchical Process\\n Create your own Manager Agent\\n Connecting to any LLM\\n Customizing Agents\\n Coding Agents\\n Forcing Tool Output as Result\\n Human Input on Execution\\n Kickoff a Crew Asynchronously\\n Kickoff a Crew for a List\\n Replay from a specific task from a kickoff\\n Conditional Tasks\\n Agent Monitoring with AgentOps\\n Agent Monitoring with LangTrace\\n Tools Docs\\n Tools Docs\\n Google Serper Search\\n Browserbase Web Loader\\n Composio Tools\\n Code Interpreter\\n Scrape Website\\n Directory Read\\n Exa Serch Web Loader\\n File Read\\n Selenium Scraper\\n Directory RAG Search\\n PDF RAG Search\\n TXT RAG Search\\n CSV RAG Search\\n XML RAG Search\\n JSON RAG Search\\n Docx Rag Search\\n MDX RAG Search\\n PG RAG Search\\n PG RAG Search\\n Table of contents\\n Description\\n Installation\\n Example Usage\\n Arguments\\n Custom Model and Embeddings\\n Website RAG Search\\n Github RAG Search\\n Code Docs RAG Search\\n Youtube Video RAG Search\\n Youtube Channel RAG Search\\n Examples\\n Examples\\n Trip Planner Crew\\n Create Instagram Post\\n Stock Analysis\\n Game Generator\\n Drafting emails with LangGraph\\n Landing Page Generator\\n Prepare for meetings\\n Telemetry\\n Change Log\\nPGSearchTool¶\\nUnder Development\\nThe PGSearchTool is currently under development. This document outlines the intended functionality and interface. As development progresses, please be aware that some features may not be available or could change.\\nDescription¶\\nThe PGSearchTool is envisioned as a powerful tool for facilitating semantic searches within PostgreSQL database tables. By leveraging advanced Retrieve and Generate (RAG) technology, it aims to provide an efficient means for querying database table content, specifically tailored for PostgreSQL databases. The tool\\'s goal is to simplify the process of finding relevant data through semantic search queries, offering a valuable resource for users needing to conduct advanced queries on extensive datasets within a PostgreSQL environment.\\nInstallation¶\\nThe crewai_tools package, which will include the PGSearchTool upon its release, can be installed using the following command:\\npip install \\'crewai[tools]\\'\\n(Note: The PGSearchTool is not yet available in the current version of the crewai_tools package. This installation command will be updated once the tool is released.)\\nExample Usage¶\\nBelow is a proposed example showcasing how to use the PGSearchTool for conducting a semantic search on a table within a PostgreSQL database:\\nfrom crewai_tools import PGSearchTool\\n# Initialize the tool with the database URI and the target table name\\ntool = PGSearchTool(db_uri=\\'postgresql://user:password@localhost:5432/mydatabase\\', table_name=\\'employees\\')\\nArguments¶\\nThe PGSearchTool is designed to require the following arguments for its operation:\\ndb_uri: A string representing the URI of the PostgreSQL database to be queried. This argument will be mandatory and must include the necessary authentication details and the location of the database.\\ntable_name: A string specifying the name of the table within the database on which the semantic search will be performed. This argument will also be mandatory.\\nCustom Model and Embeddings¶\\nThe tool intends to use OpenAI for both embeddings and summarization by default. Users will have the option to customize the model using a config dictionary as follows:\\ntool = PGSearchTool(\\n config=dict(\\n llm=dict(\\n provider=\"ollama\", # or google, openai, anthropic, llama2, ...\\n config=dict(\\n model=\"llama2\",\\n # temperature=0.5,\\n # top_p=1,\\n # stream=true,\\n ),\\n ),\\n embedder=dict(\\n provider=\"google\", # or openai, ollama, ...\\n config=dict(\\n model=\"models/embedding-001\",\\n task_type=\"retrieval_document\",\\n # title=\"Embeddings\",\\n ),\\n ),\\n )\\n)\\n Back to top\\n Previous\\n MDX RAG Search\\n Next\\n Website RAG Search\\n Copyright © 2024 crewAI, Inc\\n Made with\\n Material for MkDocs'},\n",
       " {'url': 'https://docs.crewai.com/how-to/Force-Tool-Ouput-as-Result/',\n",
       "  'content': 'Forcing Tool Output as Result - crewAI\\n Skip to content\\n crewAI\\n Forcing Tool Output as Result\\n Initializing search\\n crewAI\\n crewAI\\n crewAI\\n Home\\n Core Concepts\\n Core Concepts\\n Agents\\n Tasks\\n Tools\\n Processes\\n Crews\\n Collaboration\\n Training\\n Memory\\n Planning\\n Testing\\n Using LangChain Tools\\n Using LlamaIndex Tools\\n How to Guides\\n How to Guides\\n Starting Your crewAI Project\\n Installing CrewAI\\n Getting Started\\n Create Custom Tools\\n Using Sequential Process\\n Using Hierarchical Process\\n Create your own Manager Agent\\n Connecting to any LLM\\n Customizing Agents\\n Coding Agents\\n Forcing Tool Output as Result\\n Forcing Tool Output as Result\\n Table of contents\\n Introduction\\n Forcing Tool Output as Result\\n Workflow in Action\\n Human Input on Execution\\n Kickoff a Crew Asynchronously\\n Kickoff a Crew for a List\\n Replay from a specific task from a kickoff\\n Conditional Tasks\\n Agent Monitoring with AgentOps\\n Agent Monitoring with LangTrace\\n Tools Docs\\n Tools Docs\\n Google Serper Search\\n Browserbase Web Loader\\n Composio Tools\\n Code Interpreter\\n Scrape Website\\n Directory Read\\n Exa Serch Web Loader\\n File Read\\n Selenium Scraper\\n Directory RAG Search\\n PDF RAG Search\\n TXT RAG Search\\n CSV RAG Search\\n XML RAG Search\\n JSON RAG Search\\n Docx Rag Search\\n MDX RAG Search\\n PG RAG Search\\n Website RAG Search\\n Github RAG Search\\n Code Docs RAG Search\\n Youtube Video RAG Search\\n Youtube Channel RAG Search\\n Examples\\n Examples\\n Trip Planner Crew\\n Create Instagram Post\\n Stock Analysis\\n Game Generator\\n Drafting emails with LangGraph\\n Landing Page Generator\\n Prepare for meetings\\n Telemetry\\n Change Log\\nForcing Tool Output as Result\\nIntroduction¶\\nIn CrewAI, you can force the output of a tool as the result of an agent\\'s task. This feature is useful when you want to ensure that the tool output is captured and returned as the task result, and avoid the agent modifying the output during the task execution.\\nForcing Tool Output as Result¶\\nTo force the tool output as the result of an agent\\'s task, you can set the force_tool_output parameter to True when creating the task. This parameter ensures that the tool output is captured and returned as the task result, without any modifications by the agent.\\nHere\\'s an example of how to force the tool output as the result of an agent\\'s task:\\n# ...\\n# Define a custom tool that returns the result as the answer\\ncoding_agent =Agent(\\n role=\"Data Scientist\",\\n goal=\"Product amazing resports on AI\",\\n backstory=\"You work with data and AI\",\\n tools=[MyCustomTool(result_as_answer=True)],\\n )\\n# ...\\nWorkflow in Action¶\\nTask Execution: The agent executes the task using the tool provided.\\nTool Output: The tool generates the output, which is captured as the task result.\\nAgent Interaction: The agent my reflect and take learnings from the tool but the output is not modified.\\nResult Return: The tool output is returned as the task result without any modifications.\\n Back to top\\n Previous\\n Coding Agents\\n Next\\n Human Input on Execution\\n Copyright © 2024 crewAI, Inc\\n Made with\\n Material for MkDocs'},\n",
       " {'url': 'https://twitter.com/joaomdmoura', 'content': 'x.com'},\n",
       " {'url': 'https://docs.crewai.com/tools/BrowserbaseLoadTool/',\n",
       "  'content': 'Browserbase Web Loader - crewAI\\n Skip to content\\n crewAI\\n Browserbase Web Loader\\n Initializing search\\n crewAI\\n crewAI\\n crewAI\\n Home\\n Core Concepts\\n Core Concepts\\n Agents\\n Tasks\\n Tools\\n Processes\\n Crews\\n Collaboration\\n Training\\n Memory\\n Planning\\n Testing\\n Using LangChain Tools\\n Using LlamaIndex Tools\\n How to Guides\\n How to Guides\\n Starting Your crewAI Project\\n Installing CrewAI\\n Getting Started\\n Create Custom Tools\\n Using Sequential Process\\n Using Hierarchical Process\\n Create your own Manager Agent\\n Connecting to any LLM\\n Customizing Agents\\n Coding Agents\\n Forcing Tool Output as Result\\n Human Input on Execution\\n Kickoff a Crew Asynchronously\\n Kickoff a Crew for a List\\n Replay from a specific task from a kickoff\\n Conditional Tasks\\n Agent Monitoring with AgentOps\\n Agent Monitoring with LangTrace\\n Tools Docs\\n Tools Docs\\n Google Serper Search\\n Browserbase Web Loader\\n Browserbase Web Loader\\n Table of contents\\n Description\\n Installation\\n Example\\n Arguments\\n Composio Tools\\n Code Interpreter\\n Scrape Website\\n Directory Read\\n Exa Serch Web Loader\\n File Read\\n Selenium Scraper\\n Directory RAG Search\\n PDF RAG Search\\n TXT RAG Search\\n CSV RAG Search\\n XML RAG Search\\n JSON RAG Search\\n Docx Rag Search\\n MDX RAG Search\\n PG RAG Search\\n Website RAG Search\\n Github RAG Search\\n Code Docs RAG Search\\n Youtube Video RAG Search\\n Youtube Channel RAG Search\\n Examples\\n Examples\\n Trip Planner Crew\\n Create Instagram Post\\n Stock Analysis\\n Game Generator\\n Drafting emails with LangGraph\\n Landing Page Generator\\n Prepare for meetings\\n Telemetry\\n Change Log\\nBrowserbaseLoadTool¶\\nDescription¶\\nBrowserbase is a developer platform to reliably run, manage, and monitor headless browsers.\\nPower your AI data retrievals with:\\n - Serverless Infrastructure providing reliable browsers to extract data from complex UIs\\n - Stealth Mode with included fingerprinting tactics and automatic captcha solving\\n - Session Debugger to inspect your Browser Session with networks timeline and logs\\n - Live Debug to quickly debug your automation\\nInstallation¶\\nGet an API key and Project ID from browserbase.com and set it in environment variables (BROWSERBASE_API_KEY, BROWSERBASE_PROJECT_ID).\\nInstall the Browserbase SDK along with crewai[tools] package:\\npip install browserbase \\'crewai[tools]\\'\\nExample¶\\nUtilize the BrowserbaseLoadTool as follows to allow your agent to load websites:\\nfrom crewai_tools import BrowserbaseLoadTool\\ntool = BrowserbaseLoadTool()\\nArguments¶\\napi_key Optional. Browserbase API key. Default is BROWSERBASE_API_KEY env variable.\\nproject_id Optional. Browserbase Project ID. Default is BROWSERBASE_PROJECT_ID env variable.\\ntext_content Retrieve only text content. Default is False.\\nsession_id Optional. Provide an existing Session ID.\\nproxy Optional. Enable/Disable Proxies.\"\\n Back to top\\n Previous\\n Google Serper Search\\n Next\\n Composio Tools\\n Copyright © 2024 crewAI, Inc\\n Made with\\n Material for MkDocs'},\n",
       " {'url': 'https://github.com/joaomdmoura/crewAI-examples/tree/main/landing_page_generator',\n",
       "  'content': 'crewAI-examples/landing_page_generator at main · crewAIInc/crewAI-examples · GitHub\\nSkip to content\\nNavigation Menu\\nToggle navigation\\n Sign in\\n Product\\nActions\\n Automate any workflow\\nPackages\\n Host and manage packages\\nSecurity\\n Find and fix vulnerabilities\\nCodespaces\\n Instant dev environments\\nGitHub Copilot\\n Write better code with AI\\nCode review\\n Manage code changes\\nIssues\\n Plan and track work\\nDiscussions\\n Collaborate outside of code\\nExplore\\n All features\\n Documentation\\n GitHub Skills\\n Blog\\n Solutions\\nBy size\\n Enterprise\\n Teams\\n Startups\\nBy industry\\n Healthcare\\n Financial services\\n Manufacturing\\nBy use case\\n CI/CD & Automation\\n DevOps\\n DevSecOps\\n Resources\\nTopics\\n AI\\n DevOps\\n Security\\n Software Development\\nExplore\\n Learning Pathways\\n White papers, Ebooks, Webinars\\n Customer Stories\\n Partners\\n Open Source\\nGitHub Sponsors\\n Fund open source developers\\nThe ReadME Project\\n GitHub community articles\\nRepositories\\n Topics\\n Trending\\n Collections\\n Enterprise\\nEnterprise platform\\n AI-powered developer platform\\nAvailable add-ons\\nAdvanced Security\\n Enterprise-grade security features\\nGitHub Copilot\\n Enterprise-grade AI features\\nPremium Support\\n Enterprise-grade 24/7 support\\nPricing\\nSearch or jump to...\\nSearch code, repositories, users, issues, pull requests...\\n Search\\nClear\\n Search syntax tips\\n Provide feedback\\nWe read every piece of feedback, and take your input very seriously.\\nInclude my email address so I can be contacted\\n Cancel\\n Submit feedback\\n Saved searches\\nUse saved searches to filter your results more quickly\\nName\\nQuery\\n To see all available qualifiers, see our documentation.\\n Cancel\\n Create saved search\\n Sign in\\n Sign up\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\nDismiss alert\\n crewAIInc\\n/\\ncrewAI-examples\\nPublic\\nNotifications\\n You must be signed in to change notification settings\\nFork\\n 853\\n Star\\n 2.4k\\nCode\\nIssues\\n48\\nPull requests\\n43\\nActions\\nProjects\\n0\\nSecurity\\nInsights\\nAdditional navigation options\\n Code\\n Issues\\n Pull requests\\n Actions\\n Projects\\n Security\\n Insights\\nFooter\\n © 2024 GitHub,\\xa0Inc.\\nFooter navigation\\nTerms\\nPrivacy\\nSecurity\\nStatus\\nDocs\\nContact\\n Manage cookies\\n Do not share my personal information\\n You can’t perform that action at this time.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [dict['content'] for dict in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PG RAG Search - crewAI\\n Skip to content\\n crewAI\\n PG RAG Search\\n Initializing search\\n crewAI\\n crewAI\\n crewAI\\n Home\\n Core Concepts\\n Core Concepts\\n Agents\\n Tasks\\n Tools\\n Processes\\n Crews\\n Collaboration\\n Training\\n Memory\\n Planning\\n Testing\\n Using LangChain Tools\\n Using LlamaIndex Tools\\n How to Guides\\n How to Guides\\n Starting Your crewAI Project\\n Installing CrewAI\\n Getting Started\\n Create Custom Tools\\n Using Sequential Process\\n Using Hierarchical Process\\n Create your own Manager Agent\\n Connecting to any LLM\\n Customizing Agents\\n Coding Agents\\n Forcing Tool Output as Result\\n Human Input on Execution\\n Kickoff a Crew Asynchronously\\n Kickoff a Crew for a List\\n Replay from a specific task from a kickoff\\n Conditional Tasks\\n Agent Monitoring with AgentOps\\n Agent Monitoring with LangTrace\\n Tools Docs\\n Tools Docs\\n Google Serper Search\\n Browserbase Web Loader\\n Composio Tools\\n Code Interpreter\\n Scrape Website\\n Directory Read\\n Exa Serch Web Loader\\n File Read\\n Selenium Scraper\\n Directory RAG Search\\n PDF RAG Search\\n TXT RAG Search\\n CSV RAG Search\\n XML RAG Search\\n JSON RAG Search\\n Docx Rag Search\\n MDX RAG Search\\n PG RAG Search\\n PG RAG Search\\n Table of contents\\n Description\\n Installation\\n Example Usage\\n Arguments\\n Custom Model and Embeddings\\n Website RAG Search\\n Github RAG Search\\n Code Docs RAG Search\\n Youtube Video RAG Search\\n Youtube Channel RAG Search\\n Examples\\n Examples\\n Trip Planner Crew\\n Create Instagram Post\\n Stock Analysis\\n Game Generator\\n Drafting emails with LangGraph\\n Landing Page Generator\\n Prepare for meetings\\n Telemetry\\n Change Log\\nPGSearchTool¶\\nUnder Development\\nThe PGSearchTool is currently under development. This document outlines the intended functionality and interface. As development progresses, please be aware that some features may not be available or could change.\\nDescription¶\\nThe PGSearchTool is envisioned as a powerful tool for facilitating semantic searches within PostgreSQL database tables. By leveraging advanced Retrieve and Generate (RAG) technology, it aims to provide an efficient means for querying database table content, specifically tailored for PostgreSQL databases. The tool\\'s goal is to simplify the process of finding relevant data through semantic search queries, offering a valuable resource for users needing to conduct advanced queries on extensive datasets within a PostgreSQL environment.\\nInstallation¶\\nThe crewai_tools package, which will include the PGSearchTool upon its release, can be installed using the following command:\\npip install \\'crewai[tools]\\'\\n(Note: The PGSearchTool is not yet available in the current version of the crewai_tools package. This installation command will be updated once the tool is released.)\\nExample Usage¶\\nBelow is a proposed example showcasing how to use the PGSearchTool for conducting a semantic search on a table within a PostgreSQL database:\\nfrom crewai_tools import PGSearchTool\\n# Initialize the tool with the database URI and the target table name\\ntool = PGSearchTool(db_uri=\\'postgresql://user:password@localhost:5432/mydatabase\\', table_name=\\'employees\\')\\nArguments¶\\nThe PGSearchTool is designed to require the following arguments for its operation:\\ndb_uri: A string representing the URI of the PostgreSQL database to be queried. This argument will be mandatory and must include the necessary authentication details and the location of the database.\\ntable_name: A string specifying the name of the table within the database on which the semantic search will be performed. This argument will also be mandatory.\\nCustom Model and Embeddings¶\\nThe tool intends to use OpenAI for both embeddings and summarization by default. Users will have the option to customize the model using a config dictionary as follows:\\ntool = PGSearchTool(\\n config=dict(\\n llm=dict(\\n provider=\"ollama\", # or google, openai, anthropic, llama2, ...\\n config=dict(\\n model=\"llama2\",\\n # temperature=0.5,\\n # top_p=1,\\n # stream=true,\\n ),\\n ),\\n embedder=dict(\\n provider=\"google\", # or openai, ollama, ...\\n config=dict(\\n model=\"models/embedding-001\",\\n task_type=\"retrieval_document\",\\n # title=\"Embeddings\",\\n ),\\n ),\\n )\\n)\\n Back to top\\n Previous\\n MDX RAG Search\\n Next\\n Website RAG Search\\n Copyright © 2024 crewAI, Inc\\n Made with\\n Material for MkDocs',\n",
       " 'Forcing Tool Output as Result - crewAI\\n Skip to content\\n crewAI\\n Forcing Tool Output as Result\\n Initializing search\\n crewAI\\n crewAI\\n crewAI\\n Home\\n Core Concepts\\n Core Concepts\\n Agents\\n Tasks\\n Tools\\n Processes\\n Crews\\n Collaboration\\n Training\\n Memory\\n Planning\\n Testing\\n Using LangChain Tools\\n Using LlamaIndex Tools\\n How to Guides\\n How to Guides\\n Starting Your crewAI Project\\n Installing CrewAI\\n Getting Started\\n Create Custom Tools\\n Using Sequential Process\\n Using Hierarchical Process\\n Create your own Manager Agent\\n Connecting to any LLM\\n Customizing Agents\\n Coding Agents\\n Forcing Tool Output as Result\\n Forcing Tool Output as Result\\n Table of contents\\n Introduction\\n Forcing Tool Output as Result\\n Workflow in Action\\n Human Input on Execution\\n Kickoff a Crew Asynchronously\\n Kickoff a Crew for a List\\n Replay from a specific task from a kickoff\\n Conditional Tasks\\n Agent Monitoring with AgentOps\\n Agent Monitoring with LangTrace\\n Tools Docs\\n Tools Docs\\n Google Serper Search\\n Browserbase Web Loader\\n Composio Tools\\n Code Interpreter\\n Scrape Website\\n Directory Read\\n Exa Serch Web Loader\\n File Read\\n Selenium Scraper\\n Directory RAG Search\\n PDF RAG Search\\n TXT RAG Search\\n CSV RAG Search\\n XML RAG Search\\n JSON RAG Search\\n Docx Rag Search\\n MDX RAG Search\\n PG RAG Search\\n Website RAG Search\\n Github RAG Search\\n Code Docs RAG Search\\n Youtube Video RAG Search\\n Youtube Channel RAG Search\\n Examples\\n Examples\\n Trip Planner Crew\\n Create Instagram Post\\n Stock Analysis\\n Game Generator\\n Drafting emails with LangGraph\\n Landing Page Generator\\n Prepare for meetings\\n Telemetry\\n Change Log\\nForcing Tool Output as Result\\nIntroduction¶\\nIn CrewAI, you can force the output of a tool as the result of an agent\\'s task. This feature is useful when you want to ensure that the tool output is captured and returned as the task result, and avoid the agent modifying the output during the task execution.\\nForcing Tool Output as Result¶\\nTo force the tool output as the result of an agent\\'s task, you can set the force_tool_output parameter to True when creating the task. This parameter ensures that the tool output is captured and returned as the task result, without any modifications by the agent.\\nHere\\'s an example of how to force the tool output as the result of an agent\\'s task:\\n# ...\\n# Define a custom tool that returns the result as the answer\\ncoding_agent =Agent(\\n role=\"Data Scientist\",\\n goal=\"Product amazing resports on AI\",\\n backstory=\"You work with data and AI\",\\n tools=[MyCustomTool(result_as_answer=True)],\\n )\\n# ...\\nWorkflow in Action¶\\nTask Execution: The agent executes the task using the tool provided.\\nTool Output: The tool generates the output, which is captured as the task result.\\nAgent Interaction: The agent my reflect and take learnings from the tool but the output is not modified.\\nResult Return: The tool output is returned as the task result without any modifications.\\n Back to top\\n Previous\\n Coding Agents\\n Next\\n Human Input on Execution\\n Copyright © 2024 crewAI, Inc\\n Made with\\n Material for MkDocs',\n",
       " 'x.com',\n",
       " 'Browserbase Web Loader - crewAI\\n Skip to content\\n crewAI\\n Browserbase Web Loader\\n Initializing search\\n crewAI\\n crewAI\\n crewAI\\n Home\\n Core Concepts\\n Core Concepts\\n Agents\\n Tasks\\n Tools\\n Processes\\n Crews\\n Collaboration\\n Training\\n Memory\\n Planning\\n Testing\\n Using LangChain Tools\\n Using LlamaIndex Tools\\n How to Guides\\n How to Guides\\n Starting Your crewAI Project\\n Installing CrewAI\\n Getting Started\\n Create Custom Tools\\n Using Sequential Process\\n Using Hierarchical Process\\n Create your own Manager Agent\\n Connecting to any LLM\\n Customizing Agents\\n Coding Agents\\n Forcing Tool Output as Result\\n Human Input on Execution\\n Kickoff a Crew Asynchronously\\n Kickoff a Crew for a List\\n Replay from a specific task from a kickoff\\n Conditional Tasks\\n Agent Monitoring with AgentOps\\n Agent Monitoring with LangTrace\\n Tools Docs\\n Tools Docs\\n Google Serper Search\\n Browserbase Web Loader\\n Browserbase Web Loader\\n Table of contents\\n Description\\n Installation\\n Example\\n Arguments\\n Composio Tools\\n Code Interpreter\\n Scrape Website\\n Directory Read\\n Exa Serch Web Loader\\n File Read\\n Selenium Scraper\\n Directory RAG Search\\n PDF RAG Search\\n TXT RAG Search\\n CSV RAG Search\\n XML RAG Search\\n JSON RAG Search\\n Docx Rag Search\\n MDX RAG Search\\n PG RAG Search\\n Website RAG Search\\n Github RAG Search\\n Code Docs RAG Search\\n Youtube Video RAG Search\\n Youtube Channel RAG Search\\n Examples\\n Examples\\n Trip Planner Crew\\n Create Instagram Post\\n Stock Analysis\\n Game Generator\\n Drafting emails with LangGraph\\n Landing Page Generator\\n Prepare for meetings\\n Telemetry\\n Change Log\\nBrowserbaseLoadTool¶\\nDescription¶\\nBrowserbase is a developer platform to reliably run, manage, and monitor headless browsers.\\nPower your AI data retrievals with:\\n - Serverless Infrastructure providing reliable browsers to extract data from complex UIs\\n - Stealth Mode with included fingerprinting tactics and automatic captcha solving\\n - Session Debugger to inspect your Browser Session with networks timeline and logs\\n - Live Debug to quickly debug your automation\\nInstallation¶\\nGet an API key and Project ID from browserbase.com and set it in environment variables (BROWSERBASE_API_KEY, BROWSERBASE_PROJECT_ID).\\nInstall the Browserbase SDK along with crewai[tools] package:\\npip install browserbase \\'crewai[tools]\\'\\nExample¶\\nUtilize the BrowserbaseLoadTool as follows to allow your agent to load websites:\\nfrom crewai_tools import BrowserbaseLoadTool\\ntool = BrowserbaseLoadTool()\\nArguments¶\\napi_key Optional. Browserbase API key. Default is BROWSERBASE_API_KEY env variable.\\nproject_id Optional. Browserbase Project ID. Default is BROWSERBASE_PROJECT_ID env variable.\\ntext_content Retrieve only text content. Default is False.\\nsession_id Optional. Provide an existing Session ID.\\nproxy Optional. Enable/Disable Proxies.\"\\n Back to top\\n Previous\\n Google Serper Search\\n Next\\n Composio Tools\\n Copyright © 2024 crewAI, Inc\\n Made with\\n Material for MkDocs',\n",
       " 'crewAI-examples/landing_page_generator at main · crewAIInc/crewAI-examples · GitHub\\nSkip to content\\nNavigation Menu\\nToggle navigation\\n Sign in\\n Product\\nActions\\n Automate any workflow\\nPackages\\n Host and manage packages\\nSecurity\\n Find and fix vulnerabilities\\nCodespaces\\n Instant dev environments\\nGitHub Copilot\\n Write better code with AI\\nCode review\\n Manage code changes\\nIssues\\n Plan and track work\\nDiscussions\\n Collaborate outside of code\\nExplore\\n All features\\n Documentation\\n GitHub Skills\\n Blog\\n Solutions\\nBy size\\n Enterprise\\n Teams\\n Startups\\nBy industry\\n Healthcare\\n Financial services\\n Manufacturing\\nBy use case\\n CI/CD & Automation\\n DevOps\\n DevSecOps\\n Resources\\nTopics\\n AI\\n DevOps\\n Security\\n Software Development\\nExplore\\n Learning Pathways\\n White papers, Ebooks, Webinars\\n Customer Stories\\n Partners\\n Open Source\\nGitHub Sponsors\\n Fund open source developers\\nThe ReadME Project\\n GitHub community articles\\nRepositories\\n Topics\\n Trending\\n Collections\\n Enterprise\\nEnterprise platform\\n AI-powered developer platform\\nAvailable add-ons\\nAdvanced Security\\n Enterprise-grade security features\\nGitHub Copilot\\n Enterprise-grade AI features\\nPremium Support\\n Enterprise-grade 24/7 support\\nPricing\\nSearch or jump to...\\nSearch code, repositories, users, issues, pull requests...\\n Search\\nClear\\n Search syntax tips\\n Provide feedback\\nWe read every piece of feedback, and take your input very seriously.\\nInclude my email address so I can be contacted\\n Cancel\\n Submit feedback\\n Saved searches\\nUse saved searches to filter your results more quickly\\nName\\nQuery\\n To see all available qualifiers, see our documentation.\\n Cancel\\n Create saved search\\n Sign in\\n Sign up\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\nDismiss alert\\n crewAIInc\\n/\\ncrewAI-examples\\nPublic\\nNotifications\\n You must be signed in to change notification settings\\nFork\\n 853\\n Star\\n 2.4k\\nCode\\nIssues\\n48\\nPull requests\\n43\\nActions\\nProjects\\n0\\nSecurity\\nInsights\\nAdditional navigation options\\n Code\\n Issues\\n Pull requests\\n Actions\\n Projects\\n Security\\n Insights\\nFooter\\n © 2024 GitHub,\\xa0Inc.\\nFooter navigation\\nTerms\\nPrivacy\\nSecurity\\nStatus\\nDocs\\nContact\\n Manage cookies\\n Do not share my personal information\\n You can’t perform that action at this time.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "db = vectordb.from_texts(texts, embedding)\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='PG RAG Search - crewAI\\n Skip to content\\n crewAI\\n PG RAG Search\\n Initializing search\\n crewAI\\n crewAI\\n crewAI\\n Home\\n Core Concepts\\n Core Concepts\\n Agents\\n Tasks\\n Tools\\n Processes\\n Crews\\n Collaboration\\n Training\\n Memory\\n Planning\\n Testing\\n Using LangChain Tools\\n Using LlamaIndex Tools\\n How to Guides\\n How to Guides\\n Starting Your crewAI Project\\n Installing CrewAI\\n Getting Started\\n Create Custom Tools\\n Using Sequential Process\\n Using Hierarchical Process\\n Create your own Manager Agent\\n Connecting to any LLM\\n Customizing Agents\\n Coding Agents\\n Forcing Tool Output as Result\\n Human Input on Execution\\n Kickoff a Crew Asynchronously\\n Kickoff a Crew for a List\\n Replay from a specific task from a kickoff\\n Conditional Tasks\\n Agent Monitoring with AgentOps\\n Agent Monitoring with LangTrace\\n Tools Docs\\n Tools Docs\\n Google Serper Search\\n Browserbase Web Loader\\n Composio Tools\\n Code Interpreter\\n Scrape Website\\n Directory Read\\n Exa Serch Web Loader\\n File Read\\n Selenium Scraper\\n Directory RAG Search\\n PDF RAG Search\\n TXT RAG Search\\n CSV RAG Search\\n XML RAG Search\\n JSON RAG Search\\n Docx Rag Search\\n MDX RAG Search\\n PG RAG Search\\n PG RAG Search\\n Table of contents\\n Description\\n Installation\\n Example Usage\\n Arguments\\n Custom Model and Embeddings\\n Website RAG Search\\n Github RAG Search\\n Code Docs RAG Search\\n Youtube Video RAG Search\\n Youtube Channel RAG Search\\n Examples\\n Examples\\n Trip Planner Crew\\n Create Instagram Post\\n Stock Analysis\\n Game Generator\\n Drafting emails with LangGraph\\n Landing Page Generator\\n Prepare for meetings\\n Telemetry\\n Change Log\\nPGSearchTool¶\\nUnder Development\\nThe PGSearchTool is currently under development. This document outlines the intended functionality and interface. As development progresses, please be aware that some features may not be available or could change.\\nDescription¶\\nThe PGSearchTool is envisioned as a powerful tool for facilitating semantic searches within PostgreSQL database tables. By leveraging advanced Retrieve and Generate (RAG) technology, it aims to provide an efficient means for querying database table content, specifically tailored for PostgreSQL databases. The tool\\'s goal is to simplify the process of finding relevant data through semantic search queries, offering a valuable resource for users needing to conduct advanced queries on extensive datasets within a PostgreSQL environment.\\nInstallation¶\\nThe crewai_tools package, which will include the PGSearchTool upon its release, can be installed using the following command:\\npip install \\'crewai[tools]\\'\\n(Note: The PGSearchTool is not yet available in the current version of the crewai_tools package. This installation command will be updated once the tool is released.)\\nExample Usage¶\\nBelow is a proposed example showcasing how to use the PGSearchTool for conducting a semantic search on a table within a PostgreSQL database:\\nfrom crewai_tools import PGSearchTool\\n# Initialize the tool with the database URI and the target table name\\ntool = PGSearchTool(db_uri=\\'postgresql://user:password@localhost:5432/mydatabase\\', table_name=\\'employees\\')\\nArguments¶\\nThe PGSearchTool is designed to require the following arguments for its operation:\\ndb_uri: A string representing the URI of the PostgreSQL database to be queried. This argument will be mandatory and must include the necessary authentication details and the location of the database.\\ntable_name: A string specifying the name of the table within the database on which the semantic search will be performed. This argument will also be mandatory.\\nCustom Model and Embeddings¶\\nThe tool intends to use OpenAI for both embeddings and summarization by default. Users will have the option to customize the model using a config dictionary as follows:\\ntool = PGSearchTool(\\n config=dict(\\n llm=dict(\\n provider=\"ollama\", # or google, openai, anthropic, llama2, ...\\n config=dict(\\n model=\"llama2\",\\n # temperature=0.5,\\n # top_p=1,\\n # stream=true,\\n ),\\n ),\\n embedder=dict(\\n provider=\"google\", # or openai, ollama, ...\\n config=dict(\\n model=\"models/embedding-001\",\\n task_type=\"retrieval_document\",\\n # title=\"Embeddings\",\\n ),\\n ),\\n )\\n)\\n Back to top\\n Previous\\n MDX RAG Search\\n Next\\n Website RAG Search\\n Copyright © 2024 crewAI, Inc\\n Made with\\n Material for MkDocs')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = db.similarity_search('How to use PG RAG Search ?', k=1)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To use the PG RAG Search tool, you will need to install the crewAI_tools package using the command \"pip install 'crewai[tools]'\". Once the package is installed, you can initialize the PGSearchTool with the database URI and the target table name. The tool requires the following arguments for its operation: db_uri and table_name. The db_uri argument is a string representing the URI of the PostgreSQL database to be queried, and the table_name argument is a string specifying the name of the table within the database on which the semantic search will be performed. The tool also allows for customization of the model using a config dictionary.\n",
      "User:"
     ]
    }
   ],
   "source": [
    "api_key = 'ai71-api-f65e97e6-af86-4de9-a667-dc61c1ef8c75'\n",
    "\n",
    "from ai71 import AI71\n",
    "\n",
    "AI71_API_KEY = api_key\n",
    "\n",
    "for chunk in AI71(AI71_API_KEY).chat.completions.create(\n",
    "    model=\"tiiuae/falcon-180b-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sassy assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"question: How to use PG RAG Search ? give code \\n\\n context: {context}\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "):\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, sep=\"\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " మీరు ఎలా ఉన్నారు? Mee'ru ela unnaru?###"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "AI71_BASE_URL = \"https://api.ai71.ai/v1/\"\n",
    "AI71_API_KEY = 'ai71-api-f65e97e6-af86-4de9-a667-dc61c1ef8c75'\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"tiiuae/falcon-180b-chat\",\n",
    "    api_key=AI71_API_KEY,\n",
    "    base_url=AI71_BASE_URL,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "# Simple invocation:\n",
    "# print(\n",
    "#     chat.invoke(\n",
    "#         [\n",
    "#             SystemMessage(content=\"You are a helpful assistant. You use provided context to answer user question. \"),\n",
    "#             HumanMessage(content=f\"question: How to use Agents ? give code \\n\\n context: {context}\"),\n",
    "#         ]\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# Streaming invocation:\n",
    "for chunk in llm.stream(\"translate to telugu, 'How are you'\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To use the PG RAG Search, you will need to install the crewai_tools package using pip. Once installed, you can initialize the PGSearchTool with the database URI and target table name as arguments. The tool will use OpenAI for embeddings and summarization by default, but users can also provide their own models and configurations.\n",
      "\n",
      "Here is a sample code to use PG RAG Search:\n",
      "\n",
      "```python\n",
      "from crewai_tools import PGSearchTool\n",
      "# Initialize the tool with the database URI and the target table name\n",
      "tool = PGSearchTool(db_uri='postgresql://user:password@localhost:5432/mydatabase', table_name='employees')\n",
      "\n",
      "# Customize the model and embeddings if needed\n",
      "config = dict(\n",
      "    llm=dict(\n",
      "        provider=\"ollama\", # or google, openai, anthropic, llama2, ...\n",
      "        config=dict(\n",
      "            model=\"llama2\",\n",
      "            # temperature=0.5,\n",
      "            # top_p=1,\n",
      "            # stream=True,\n",
      "        ),\n",
      "    ),\n",
      "    embedder=dict(\n",
      "        provider=\"google\", # or openai, ollama, ...\n",
      "        config=dict(\n",
      "            model=\"models/embedding-001\",\n",
      "            # title=\"Embeddings\",\n",
      "            task_type=\"retrieval_document\",\n",
      "        ),\n",
      "    ),\n",
      ")\n",
      "tool = PGSearchTool(db_uri='postgresql://user:password@localhost:5432/mydatabase', table_name='employees', config=config)\n",
      "\n",
      "# Conduct a semantic search on the employees table\n",
      "results = tool.search('query')\n",
      "```\n",
      "\n",
      "Note that the PGSearchTool is currently under development and some features may not be available yet. For more information, please refer to the crewAI documentation.\n",
      "User:"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "AI71_BASE_URL = \"https://api.ai71.ai/v1/\"\n",
    "AI71_API_KEY = api_key\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=AI71_API_KEY,\n",
    "    base_url=AI71_BASE_URL,\n",
    ")\n",
    "\n",
    "# Simple invocation:\n",
    "# print(client.chat.completions.create(\n",
    "#     model=\"tiiuae/falcon-180b-chat\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "#     ],\n",
    "# ))\n",
    "\n",
    "# Streaming invocation:\n",
    "for chunk in client.chat.completions.create(\n",
    "    messages=[{\n",
    "      \"role\": \"user\", \n",
    "      \"content\": f\"question: How to use PG RAG Search ? give code \\n\\n context: {context}\"\n",
    "    }],\n",
    "    model=\"tiiuae/falcon-180b-chat\",\n",
    "    stream=True,\n",
    "    temperature=0.9,\n",
    "):\n",
    "    # print(chunk)\n",
    "    delta_content = chunk.choices[0].delta.content\n",
    "    if delta_content:\n",
    "        print(delta_content, sep=\"\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://ollama.com/blog/how-to-prompt-code-llama\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/events\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/llama-3-is-not-very-censored\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/llama3\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/tool-support\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/python-javascript-libraries\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/run-code-llama-locally\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/gemma2\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/continue-code-assistant\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/building-llm-powered-web-apps\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/library\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/login\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/\n",
      "Using Tool: Read website content\n",
      "Scraping https://github.com/ollama/ollama/tree/main/docs\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/firebase-genkit\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/openai-compatibility\n",
      "Using Tool: Read website content\n",
      "Scraping https://discord.com/invite/ollama\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/vision-models\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/download\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/search\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/run-llama2-uncensored-locally\n",
      "Using Tool: Read website content\n",
      "Scraping https://github.com/ollama/ollama\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/embedding-models\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/llms-in-obsidian\n",
      "Using Tool: Read website content\n",
      "Scraping https://twitter.com/ollama\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/amd-preview\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/wizardmath-examples\n",
      "Using Tool: Read website content\n",
      "Scraping https://ollama.com/blog/windows-preview\n",
      "Using Tool: Read website content\n"
     ]
    }
   ],
   "source": [
    "from fns import extract_content\n",
    "\n",
    "content = extract_content('https://ollama.com/blog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\n",
    "\n",
    "for dict in content:\n",
    "    corpus += 'url: ' + dict['url'] + '\\n' + 'content: ' + dict['content'] + '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_text(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prudh\\anaconda3\\envs\\gen_ai1\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_texts(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = db.similarity_search('lamma', k=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='“Who is this about?”\\nhttps://smith.langchain.com/public/2386b1de-7afb-48a2-8c83-205162bfcac0/r\\n“Do they know JavaScript?”\\nhttps://smith.langchain.com/public/18cec162-d12c-4034-aa9a-39b1cd2011ea/r\\nConclusions\\nOverall, this worked out well. A few observations:\\nOpen source models are advancing rapidly - I built the initial version of this app with Llama 2, and Mistral was announced just weeks later.\\nMore and more consumer hardware manufacturers are including GPUs in their products.\\nAs OSS models get smaller and faster, running these models on local hardware with tools like Ollama becomes will become more and more common.\\nWhile browser-friendly tech for vectorstores, embeddings, and other task-specific models has undergone some incredible advancements in the last few months, LLMs are still far too large to feasibly ship bundled in web apps.'),\n",
       " Document(page_content='© 2024 Ollama\\nBlog\\nDocs\\nGitHub\\nDiscord\\nX (Twitter)\\nMeetups\\nBlog\\nDocs\\nGitHub\\nDiscord\\nX (Twitter)\\nMeetups\\n © 2024 Ollama'),\n",
       " Document(page_content='© 2024 Ollama\\nBlog\\nDocs\\nGitHub\\nDiscord\\nX (Twitter)\\nMeetups\\nBlog\\nDocs\\nGitHub\\nDiscord\\nX (Twitter)\\nMeetups\\n © 2024 Ollama'),\n",
       " Document(page_content='© 2024 Ollama\\nBlog\\nDocs\\nGitHub\\nDiscord\\nX (Twitter)\\nMeetups\\nBlog\\nDocs\\nGitHub\\nDiscord\\nX (Twitter)\\nMeetups\\n © 2024 Ollama'),\n",
       " Document(page_content=\"Kerlig AI (AI writing assistant for macOS)\\nAI Studio\\nSidellama (browser-based LLM client)\\nLLMStack (No-code multi-agent framework to build LLM agents and workflows)\\nTerminal\\noterm\\nEllama Emacs client\\nEmacs client\\ngen.nvim\\nollama.nvim\\nollero.nvim\\nollama-chat.nvim\\nogpt.nvim\\ngptel Emacs client\\nOatmeal\\ncmdh\\nooo\\nshell-pilot\\ntenere\\nllm-ollama for Datasette's LLM CLI.\\ntypechat-cli\\nShellOracle\\ntlm\\npodman-ollama\\ngollama\\nDatabase\\nMindsDB (Connects Ollama models with nearly 200 data platforms and apps)\\nchromem-go with example\\nPackage managers\\nPacman\\nHelm Chart\\nGuix channel\\nLibraries\\nLangChain and LangChain.js with example\\nLangChainGo with example\\nLangChain4j with example\\nLangChainRust with example\\nLlamaIndex\\nLiteLLM\\nOllamaSharp for .NET\\nOllama for Ruby\\nOllama-rs for Rust\\nOllama-hpp for C++\\nOllama4j for Java\\nModelFusion Typescript Library\\nOllamaKit for Swift\\nOllama for Dart\\nOllama for Laravel\\nLangChainDart\\nSemantic Kernel - Python\\nHaystack\\nElixir LangChain\\nOllama for R - rollama\"),\n",
       " Document(page_content='Thank you LLaVA team\\nThe LLaVA 1.6 model family is published by an incredible team. Information on how the models were trained is available on the LLaVA website, as well as benchmark results comparing LLaVA 1.6 to leading open-source and proprietary models.\\n© 2024 Ollama\\nBlog\\nDocs\\nGitHub\\nDiscord\\nX (Twitter)\\nMeetups\\nBlog\\nDocs\\nGitHub\\nDiscord\\nX (Twitter)\\nMeetups\\n © 2024 Ollama'),\n",
       " Document(page_content='Examples\\nObject detection\\n% ollama run llava \"tell me what do you see in this picture? ./pic.jpg\"\\nIn the image, a man wearing blue and white is holding video game controllers in his hands while smiling. His arms are raised above his head as he plays with the virtual reality gaming devices. The man appears to be enjoying himself and immersed in his gaming experience.\\nText recognition\\n% ollama run llava \"what does the text say? ./wordart.png\"\\nThis is an image with a capitalized, bolded text of \"ollama.\" The background is black and the letters are yellow, which create a contrasting appearance. Although the words \"ollama\" make up the majority of the image, it could also be seen as part of a sign or advertisement due to its brightness and prominent font style.\\nThank you LLaVA team')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prudh\\anaconda3\\envs\\gen_ai1\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "AI71_BASE_URL = \"https://api.ai71.ai/v1/\"\n",
    "AI71_API_KEY = 'ai71-api-f65e97e6-af86-4de9-a667-dc61c1ef8c75'\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"tiiuae/falcon-180b-chat\",\n",
    "    api_key=AI71_API_KEY,\n",
    "    base_url=AI71_BASE_URL,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "# from langchain_groq import ChatGroq\n",
    "# llm = ChatGroq(model=\"llama3-70b-8192\", api_key='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')\n",
    "\n",
    "\n",
    "\n",
    "# for chunk in chat.stream(f\"question: How to use Agents ? give code \\n\\n context: {context[0].page_content[:6000]}\"):\n",
    "#     print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=db.as_retriever(k=5)\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    # max_tokens_limit = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='url: https://docs.crewai.com/tools/ScrapeWebsiteTool/\\ncontent: Scrape Website - crewAI\\n Skip to content\\n crewAI\\n Scrape Website\\n Initializing search\\n crewAI\\n crewAI\\n crewAI\\n Home\\n Core Concepts\\n Core Concepts\\n Agents\\n Tasks\\n Tools\\n Processes\\n Crews\\n Collaboration\\n Training\\n Memory\\n Planning\\n Testing\\n Using LangChain Tools\\n Using LlamaIndex Tools\\n How to Guides\\n How to Guides\\n Starting Your crewAI Project\\n Installing CrewAI\\n Getting Started\\n Create Custom Tools\\n Using Sequential Process\\n Using Hierarchical Process\\n Create your own Manager Agent\\n Connecting to any LLM\\n Customizing Agents\\n Coding Agents\\n Forcing Tool Output as Result\\n Human Input on Execution\\n Kickoff a Crew Asynchronously\\n Kickoff a Crew for a List\\n Replay from a specific task from a kickoff\\n Conditional Tasks\\n Agent Monitoring with AgentOps\\n Agent Monitoring with LangTrace\\n Tools Docs\\n Tools Docs\\n Google Serper Search\\n Browserbase Web Loader\\n Composio Tools\\n Code Interpreter\\n Scrape Website\\n Scrape Website\\n Table of contents\\n Description\\n Installation\\n Example\\n Arguments\\n Directory Read\\n Exa Serch Web Loader\\n File Read\\n Selenium Scraper\\n Directory RAG Search\\n PDF RAG Search\\n TXT RAG Search\\n CSV RAG Search\\n XML RAG Search\\n JSON RAG Search\\n Docx Rag Search\\n MDX RAG Search\\n PG RAG Search\\n Website RAG Search\\n Github RAG Search\\n Code Docs RAG Search\\n Youtube Video RAG Search\\n Youtube Channel RAG Search\\n Examples\\n Examples\\n Trip Planner Crew\\n Create Instagram Post\\n Stock Analysis\\n Game Generator'),\n",
       " Document(page_content=\"Youtube Video RAG Search\\n Youtube Channel RAG Search\\n Examples\\n Examples\\n Trip Planner Crew\\n Create Instagram Post\\n Stock Analysis\\n Game Generator\\n Drafting emails with LangGraph\\n Landing Page Generator\\n Prepare for meetings\\n Telemetry\\n Change Log\\nScrapeWebsiteTool¶\\nExperimental\\nWe are still working on improving tools, so there might be unexpected behavior or changes in the future.\\nDescription¶\\nA tool designed to extract and read the content of a specified website. It is capable of handling various types of web pages by making HTTP requests and parsing the received HTML content. This tool can be particularly useful for web scraping tasks, data collection, or extracting specific information from websites.\\nInstallation¶\\nInstall the crewai_tools package\\npip install 'crewai[tools]'\\nExample¶\\nfrom crewai_tools import ScrapeWebsiteTool\\n# To enable scrapping any website it finds during it's execution\\ntool = ScrapeWebsiteTool()\\n# Initialize the tool with the website URL, so the agent can only scrap the content of the specified website\\ntool = ScrapeWebsiteTool(website_url='https://www.example.com')\\n# Extract the text from the site\\ntext = tool.run()\\nprint(text)\\nArguments¶\\nwebsite_url : Mandatory website URL to read the file. This is the primary input for the tool, specifying which website's content should be scraped and read.\\n Back to top\\n Previous\\n Code Interpreter\\n Next\\n Directory Read\\n Copyright © 2024 crewAI, Inc\\n Made with\\n Material for MkDocs\"),\n",
       " Document(page_content=\"tool = SeleniumScrapingTool(website_url='https://example.com', css_element='.main-content')\\n# Example 4: Perform scraping with additional parameters for a customized experience\\ntool = SeleniumScrapingTool(website_url='https://example.com', css_element='.main-content', cookie={'name': 'user', 'value': 'John Doe'}, wait_time=10)\\nArguments¶\\nThe following parameters can be used to customize the SeleniumScrapingTool's scraping process:\\nwebsite_url: Mandatory. Specifies the URL of the website from which content is to be scraped.\\ncss_element: Mandatory. The CSS selector for a specific element to target on the website. This enables focused scraping of a particular part of a webpage.\\ncookie: Optional. A dictionary that contains cookie information. Useful for simulating a logged-in session, thereby providing access to content that might be restricted to non-logged-in users.\\nwait_time: Optional. Specifies the delay (in seconds) before the content is scraped. This delay allows for the website and any dynamic content to fully load, ensuring a successful scrape.\\nAttention\\nSince the SeleniumScrapingTool is under active development, the parameters and functionality may evolve over time. Users are encouraged to keep the tool updated and report any issues or suggestions for enhancements.\\n Back to top\\n Previous\\n File Read\\n Next\\n Directory RAG Search\\n Copyright © 2024 crewAI, Inc\\n Made with\\n Material for MkDocs\"),\n",
       " Document(page_content=\"Youtube Video RAG Search\\n Youtube Channel RAG Search\\n Examples\\n Examples\\n Trip Planner Crew\\n Create Instagram Post\\n Stock Analysis\\n Game Generator\\n Drafting emails with LangGraph\\n Landing Page Generator\\n Prepare for meetings\\n Telemetry\\n Change Log\\nSeleniumScrapingTool¶\\nExperimental\\nThis tool is currently in development. As we refine its capabilities, users may encounter unexpected behavior. Your feedback is invaluable to us for making improvements.\\nDescription¶\\nThe SeleniumScrapingTool is crafted for high-efficiency web scraping tasks. It allows for precise extraction of content from web pages by using CSS selectors to target specific elements. Its design caters to a wide range of scraping needs, offering flexibility to work with any provided website URL.\\nInstallation¶\\nTo get started with the SeleniumScrapingTool, install the crewai_tools package using pip:\\npip install 'crewai[tools]'\\nUsage Examples¶\\nBelow are some scenarios where the SeleniumScrapingTool can be utilized:\\nfrom crewai_tools import SeleniumScrapingTool\\n# Example 1: Initialize the tool without any parameters to scrape the current page it navigates to\\ntool = SeleniumScrapingTool()\\n# Example 2: Scrape the entire webpage of a given URL\\ntool = SeleniumScrapingTool(website_url='https://example.com')\\n# Example 3: Target and scrape a specific CSS element from a webpage\\ntool = SeleniumScrapingTool(website_url='https://example.com', css_element='.main-content')\")]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"How to scrape website using tools?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Give demo on how to use\"\n",
    "result = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here's a brief demonstration of how to use Ollama:\n",
      "\n",
      "1. First, you need to install Ollama. You can do this by running the command \"ollama install\" in your terminal.\n",
      "\n",
      "2. Once Ollama is installed, you can run it using the \"ollama run\" command followed by the name of the Llava program you want to run. For example, to run the \"tell me what do you see in this picture?\" program, you would run \"ollama run llava \\\"tell me what do you see in this picture? ./pic.jpg\\\"\".\n",
      "\n",
      "3. Ollama will then prompt you to ask a question, which should be related to the Llava program you just ran. For example, if you ran the \"tell me what do you see in this picture?\" program, you could ask \"What is in the picture?\" or \"What is the man doing in the picture?\".\n",
      "\n",
      "4. Ollama will then use its natural language processing capabilities to analyze your question and the output of the Llava program you ran, and provide you with a response. For example, if you asked \"What is in the picture?\" and the Llava program detected a man in the picture, Ollama might respond with \"There is a man in the picture.\"\n",
      "\n",
      "That's a brief overview of how to use Ollama. Of course, there are many more complex things you can do with Ollama, but hopefully this gives you a good starting point.\n",
      "User:\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'can i search youtube, give code', 'chat_history': [HumanMessage(content='How to scrate website using tools?'), AIMessage(content=\" To scrape a website using ScrapeWebsiteTool in crewAI, you need to follow these steps:\\n\\n1. Install the crewAI_tools package using pip command: `pip install 'crewai[tools]'`\\n2. Import the ScrapeWebsiteTool module in your Python script.\\n3. Initialize the ScrapeWebsiteTool object by passing the website URL as an argument. For example, `tool = ScrapeWebsiteTool(website_url='https://www.example.com')`\\n4. Run the ScrapeWebsiteTool by calling the `run()` method. For example, `text = tool.run()`.\\n5. The scraped content will be stored in the `text` variable, and you can use it for further processing or analysis.\\n\\nHere is an example code snippet that demonstrates how to use ScrapeWebsiteTool:\\n\\n```\\nfrom crewai_tools import ScrapeWebsiteTool\\n\\ntool = ScrapeWebsiteTool(website_url='https://www.example.com')\\ntext = tool.run()\\nprint(text)\\n```\\n\\nPlease note that the ScrapeWebsiteTool is still experimental, and there might be unexpected behavior or changes in the future.\\nUser:\"), HumanMessage(content='can i search youtube'), AIMessage(content=' Yes, it is possible to search YouTube using crewAI. You can use the crewAI tool called \"Youtube Video RAG Search\" or \"Youtube Channel RAG Search\" to perform semantic searches within YouTube. These tools are designed to help developers, researchers or anyone in need of precise information from YouTube. You can find more information about these tools in the crewAI documentation.\\nUser:'), HumanMessage(content='can i search youtube, give code'), AIMessage(content=' Sure! Here\\'s how you can search YouTube using crewAI tools:\\n\\nFirst, make sure you have the crewai_tools package installed:\\n\\n```\\npip install crewai\\n```\\n\\nThen, you can use the YouTubeSearchTool like this:\\n\\n```\\nfrom crewai_tools import YouTubeSearchTool\\n\\ntool = YouTubeSearchTool(search_terms=\"CrewAI tutorial\", max_results=5)\\nresults = tool.search()\\n\\nprint(results)\\n```\\n\\nThis will search YouTube for videos with the title or description containing \"CrewAI tutorial\" and return up to 5 results. The results object will contain information about each video, such as the title, URL, and thumbnail image.\\n\\nYou can also customize the search using the various parameters available in the YouTubeSearchTool class. For example, you can specify the duration of the video, the language, and the region. Check out the crewAI documentation for more information.\\nUser:')], 'answer': ' Sure! Here\\'s how you can search YouTube using crewAI tools:\\n\\nFirst, make sure you have the crewai_tools package installed:\\n\\n```\\npip install crewai\\n```\\n\\nThen, you can use the YouTubeSearchTool like this:\\n\\n```\\nfrom crewai_tools import YouTubeSearchTool\\n\\ntool = YouTubeSearchTool(search_terms=\"CrewAI tutorial\", max_results=5)\\nresults = tool.search()\\n\\nprint(results)\\n```\\n\\nThis will search YouTube for videos with the title or description containing \"CrewAI tutorial\" and return up to 5 results. The results object will contain information about each video, such as the title, URL, and thumbnail image.\\n\\nYou can also customize the search using the various parameters available in the YouTubeSearchTool class. For example, you can specify the duration of the video, the language, and the region. Check out the crewAI documentation for more information.\\nUser:'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content=\"ImportError: cannot import name 'YouTubeSearchTool' from 'crewai_tools' (c:\\\\Users\\\\prudh\\\\anaconda3\\\\envs\\\\gen_ai1\\\\lib\\\\site-packages\\\\crewai_tools\\\\__init__.py), got this error\"), AIMessage(content=' It looks like there might be a typo in the code, as the error message indicates that the module \"YouTubeSearchTool\" cannot be found, but the correct module name should be \"YoutubeVideoSearchTool\". Please double-check your code and make sure to use the correct module name.\\nUser:'), HumanMessage(content='can i search youtube, give code'), AIMessage(content=\" Sure, here's the code to search YouTube using the crewAI tools:\\n\\n```python\\nimport crew\\n\\ncrew.init()\\n\\nyoutube_search = crew.tools['YoutubeChannelSearchTool']\\n\\nchannel_id = '123456'\\nresults = youtube_search(channel_id)\\n\\nfor result in results:\\n    print(result)\\n```\\n\\nIn this example, we are using the `youtube_search` function from the `YoutubeChannelSearchTool`. We pass in the channel ID of the channel we want to search, and the function returns a list of results.\\n\\nNote that you will need to install crewAI and the `YoutubeChannelSearchTool` to be able to use this code.\\nUser:\"), HumanMessage(content=\"---------------------------------------------------------------------------\\nModuleNotFoundError                       Traceback (most recent call last)\\nCell In[20], line 1\\n----> 1 import crew\\n      3 crew.init()\\n      5 youtube_search = crew.tools['YoutubeChannelSearchTool']\\n\\nModuleNotFoundError: No module named 'crew' \\n\\n got this error\"), AIMessage(content=\" I'm sorry, as an AI language model, I don't have the capability to double-check if crewAI is installed or if it has been imported correctly. However, based on the given context, it seems like the user is trying to install crewAI. To install crewAI, the user needs to have Python >=3.10 and <=3.13 installed on their system and can use the following commands:\\n\\n```\\n# Install the main crewAI package\\npip install crewai\\n# Install the main crewAI package and the tools package\\n# that includes a series of helpful tools for your agents\\npip install 'crewai[tools]'\\n``` \\n\\nIf the installation is successful, the user can import crewAI in their Python script using the following command:\\n\\n```python\\nimport crewai\\n```\\n\\nPlease note that if the installation or import is not successful, the user may receive an error message indicating the issue.\\nUser:\")]) return_messages=True memory_key='chat_history'\n"
     ]
    }
   ],
   "source": [
    "print(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "AI71_BASE_URL = \"https://api.ai71.ai/v1/\"\n",
    "AI71_API_KEY = 'ai71-api-f65e97e6-af86-4de9-a667-dc61c1ef8c75'\n",
    "\n",
    "\n",
    "llm1 = ChatOpenAI(\n",
    "    model=\"tiiuae/falcon-180b-chat\",\n",
    "    api_key=AI71_API_KEY,\n",
    "    base_url=AI71_BASE_URL,\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='After installing, open your favorite terminal and run ollama run llama2 to run a model\\nOllama will prompt for updates as new releases become available. We’d love your feedback! If you encounter any issues please let us know by opening an issue or by joining the Discord server.\\n© 2024 Ollama\\nBlog\\nDocs\\nGitHub\\nDiscord\\nX (Twitter)\\nMeetups\\nBlog\\nDocs\\nGitHub\\nDiscord\\nX (Twitter)\\nMeetups\\n © 2024 Ollama'), Document(page_content='Examples\\nObject detection\\n% ollama run llava \"tell me what do you see in this picture? ./pic.jpg\"\\nIn the image, a man wearing blue and white is holding video game controllers in his hands while smiling. His arms are raised above his head as he plays with the virtual reality gaming devices. The man appears to be enjoying himself and immersed in his gaming experience.\\nText recognition\\n% ollama run llava \"what does the text say? ./wordart.png\"\\nThis is an image with a capitalized, bolded text of \"ollama.\" The background is black and the letters are yellow, which create a contrasting appearance. Although the words \"ollama\" make up the majority of the image, it could also be seen as part of a sign or advertisement due to its brightness and prominent font style.\\nThank you LLaVA team'), Document(page_content='url: https://discord.com/invite/ollama\\ncontent: Ollama'), Document(page_content=\"})\\nconsole.log(response.message.content)\\nUse cases\\nBoth libraries support Ollama’s full set of features. Here are some examples in Python:\\nStreaming\\nfor chunk in chat('mistral', messages=messages, stream=True):\\n print(chunk['message']['content'], end='', flush=True)\\nMulti-modal\\nwith open('image.png', 'rb') as file:\\n response = ollama.chat(\\n model='llava',\\n messages=[\\n {\\n 'role': 'user',\\n 'content': 'What is strange about this image?',\\n 'images': [file.read()],\\n },\\n ],\\n )\\nprint(response['message']['content'])\\nText Completion\\nresult = ollama.generate(\\n model='stable-code',\\n prompt='// A c function to reverse a string\\\\n',\\n)\\nprint(result['response'])\\nCreating custom models\\nmodelfile='''\\nFROM llama2\\nSYSTEM You are mario from super mario bros.\\n'''\\nollama.create(model='example', modelfile=modelfile)\\nCustom client\\nollama = Client(host='my.ollama.host')\\nMore examples are available in the GitHub repositories for the Python and JavaScript libraries.\\nNew GitHub handle\"), Document(page_content='a. Extract and load the “accepted tab suggestions” into Hugging Face Datasets\\nb. Use Hugging Face Supervised Fine-tuning Trainer to fine-tune StarCoder 2\\nc. Push the model to the Ollama model library for your team to use and measure how your acceptance rate changes\\nLearn more about Ollama by using @docs to ask questions with the help of Continue\\nContinue also comes with an @docs context provider built-in, which lets you index and retrieve snippets from any documentation site. Assuming you have a chat model set up already (e.g. Codestral, Llama 3), you can keep this entire experience local by providing a link to the Ollama README on GitHub and asking questions to learn more with it as context.\\na. Type @docs in the chat sidebar, select “Add Docs”, copy and paste “https://github.com/ollama/ollama” into the URL field, and type “Ollama” into the title field')]\n",
      " To use Ollama, you need to first install it and then run the model with the command \"ollama run llama2\". You can also use various features of Ollama such as text recognition, object detection, and text completion. Examples of using these features are provided in the given context. Additionally, you can create custom models and use them with Ollama. If you encounter any issues, you can open an issue on GitHub or join the Discord server for help. To learn more about Ollama, you can use the @docs context provider in your chat model.\n",
      "User:"
     ]
    }
   ],
   "source": [
    "question = 'Help me on how to use ollama'\n",
    "context = db.max_marginal_relevance_search(question, k=5)\n",
    "print(context)\n",
    "for chunk in llm1.stream(\"Answer the based on the context\" + f\"question: {question} \\n\\n context: {context}\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 1, 'per_page': 1, 'photos': [{'id': 24445646, 'width': 4160, 'height': 2773, 'url': 'https://www.pexels.com/photo/red-and-yellow-hibiscus-flower-24445646/', 'photographer': 'Kavya C', 'photographer_url': 'https://www.pexels.com/@kavya-c-1119716708', 'photographer_id': 1119716708, 'avg_color': '#513C1F', 'src': {'original': 'https://images.pexels.com/photos/24445646/pexels-photo-24445646.jpeg', 'large2x': 'https://images.pexels.com/photos/24445646/pexels-photo-24445646.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=650&w=940', 'large': 'https://images.pexels.com/photos/24445646/pexels-photo-24445646.jpeg?auto=compress&cs=tinysrgb&h=650&w=940', 'medium': 'https://images.pexels.com/photos/24445646/pexels-photo-24445646.jpeg?auto=compress&cs=tinysrgb&h=350', 'small': 'https://images.pexels.com/photos/24445646/pexels-photo-24445646.jpeg?auto=compress&cs=tinysrgb&h=130', 'portrait': 'https://images.pexels.com/photos/24445646/pexels-photo-24445646.jpeg?auto=compress&cs=tinysrgb&fit=crop&h=1200&w=800', 'landscape': 'https://images.pexels.com/photos/24445646/pexels-photo-24445646.jpeg?auto=compress&cs=tinysrgb&fit=crop&h=627&w=1200', 'tiny': 'https://images.pexels.com/photos/24445646/pexels-photo-24445646.jpeg?auto=compress&cs=tinysrgb&dpr=1&fit=crop&h=200&w=280'}, 'liked': False, 'alt': 'A yellow and red flower with green leaves'}], 'total_results': 8000, 'next_page': 'https://api.pexels.com/v1/search/?orientation=landscape&page=2&per_page=1&query=lord+krishna+in+garden'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the API endpoint and parameters\n",
    "url = \"https://api.pexels.com/v1/search\"\n",
    "headers = {\n",
    "    \"Authorization\": \"m6JFvfTSJPOoNnaxSmSr5UVa4oVVXUyGOgVAqDukRZEvKUOHmeSpyZoB\"\n",
    "}\n",
    "params = {\n",
    "    \"query\": \"lord krishna in garden\",\n",
    "    \"per_page\": 1,\n",
    "    \"orientation\": \"landscape\"\n",
    "}\n",
    "\n",
    "# Make the GET request\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Print the response JSON (or handle it as needed)\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output_video.mp4.\n",
      "MoviePy - Writing audio in output_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_video.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# Load your video\n",
    "clip = VideoFileClip(\"video.mp4\")\n",
    "\n",
    "# Add a fade-in effect\n",
    "fade_in_duration = 3  # duration in seconds\n",
    "# clip = clip.fadein(fade_in_duration)\n",
    "clip = clip.fx(vfx.speedx, factor=2)  # Speed up by a factor of 2\n",
    "    \n",
    "# Save the edited video\n",
    "clip.write_videofile(\"output_video.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
